# -*- coding: utf-8 -*-
"""Ecommerce.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ShMUidop4hJ3A1zpXqE0SuabloZuvy_g
"""

!pip install langchain faiss-cpu openai unstructured tiktoken scrapy

import scrapy
from pathlib import Path
from scrapy.crawler import CrawlerProcess
from scrapy.linkextractors import LinkExtractor
import csv
def crawler(domain_name, initial_URL):
    class BooksSpider(scrapy.Spider):
        name = "bookurl"
        allowed_domains = [domain_name]
        start_urls = [initial_URL]

        try:
            with open('urlbooks.csv', 'w', newline='') as csvfile:
                csv_writer = csv.writer(csvfile)
                csv_writer.writerow(['Link'])
        except OSError:
            pass

        custom_settings = {
            'CONCURRENT_REQUESTS': 18,
            'AUTOTHROTTLE_ENABLED': True,
            'AUTOTHROTTLE_DEBUG': True,
            'AUTOTHROTTLE_START_DELAY': 1.0,
            'AUTOTHROTTLE_TARGET_CONCURRENCY': 5.0,
            'DOWNLOAD_DELAY': 0.5
        }

        def __init__(self):
            self.link_extractor = LinkExtractor(unique=True)
            self.visited_urls = set()

        def closed(self, reason):
            pass

        def parse(self, response):
            for link in self.link_extractor.extract_links(response):
                url = link.url
                if url not in self.visited_urls:
                    self.visited_urls.add(url)
                    with open('urlbooks.csv', 'a', newline='') as csvfile:
                        csv_writer = csv.writer(csvfile)
                        csv_writer.writerow([url])
                    yield response.follow(url=link, callback=self.parse)
            next_page_links = response.css('ul.pagination li.next a::attr(href)').extract()
            for next_page_link in next_page_links:
                yield response.follow(url=next_page_link, callback=self.parse)

    return BooksSpider


def run_crawler(domain_name, initial_URL):
    process = CrawlerProcess()
    process.crawl(crawler(domain_name, initial_URL))
    process.start()

domain_name = input("Enter the Domain Name:")
initial_URL = input("Enter Initial URL:")
run_crawler(domain_name, initial_URL)

import pandas as pd
df = pd.read_csv("/content/urlbooks.csv")
column_list = df['Link'].tolist()
print(column_list)

from langchain.document_loaders import UnstructuredURLLoader
loaders = UnstructuredURLLoader(urls=column_list)
data = loaders.load()

from langchain.text_splitter import CharacterTextSplitter
text_splitter = CharacterTextSplitter(separator='\n', chunk_size=1000, chunk_overlap=200)
docs = text_splitter.split_documents(data)

import os
os.environ["OPENAI_API_KEY"] = "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"

from langchain.embeddings import OpenAIEmbeddings
embeddings = OpenAIEmbeddings()
embeddings

import pickle
import faiss
from langchain.vectorstores import FAISS
faiss='faiss_db'
vectorStore_openAI = FAISS.from_documents(docs, embeddings)
vectorStore_openAI.save_local(faiss)
reload_faiss=FAISS.load_local(faiss, embeddings)

reload_faiss

from langchain.chains import RetrievalQAWithSourcesChain
from langchain.chains.question_answering import load_qa_chain
from langchain import OpenAI
llm=OpenAI(temperature=0, model_name='text-davinci-003')
chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=reload_faiss.as_retriever())

chain({"question": "tell me about sharp objects book and its price and availability"}, return_only_outputs=True)

chain({"question": "books on politics"}, return_only_outputs=True)

chain({"question": "suggest a book on entrepreneurship and what is its cost and rating"}, return_only_outputs=True)

chain({"question": "suggest me a religious book which help my soul to heal"}, return_only_outputs=True)

chain({"question": "which is the most expensive fiction book "}, return_only_outputs=True)

chain({"question": "book helping me raise money for my business "}, return_only_outputs=True)

chain({"question": "give me any horror related book"}, return_only_outputs=True)